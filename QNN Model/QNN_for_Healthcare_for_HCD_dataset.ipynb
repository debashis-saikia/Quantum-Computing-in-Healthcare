{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn-oJ8sUmcKO",
        "outputId": "04a4711a-3bda-4077-a9c5-5067b46376fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install qiskit --quiet\n",
        "%pip install qiskit-aer --quiet\n",
        "%pip install pylatexenc --quiet\n",
        "%pip install qiskit-machine-learning --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install qiskit-algorithms --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LW4LUkJmmwMG",
        "outputId": "d74aa61a-0338-41ff-c031-99c536c97f18"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/310.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow import keras\n",
        "\n",
        "import qiskit\n",
        "from qiskit import QuantumCircuit, transpile\n",
        "from qiskit.transpiler import generate_preset_pass_manager\n",
        "from qiskit.visualization import plot_histogram\n",
        "from qiskit.circuit.library import RealAmplitudes, EfficientSU2, ZZFeatureMap, TwoLocal\n",
        "import pylatexenc"
      ],
      "metadata": {
        "id": "uZchaHLCnAhF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import Linear, CrossEntropyLoss, MSELoss\n",
        "from torch.optim import LBFGS\n",
        "\n",
        "from qiskit.circuit import Parameter\n",
        "from qiskit_algorithms.utils import algorithm_globals\n",
        "from qiskit_machine_learning.neural_networks import SamplerQNN, EstimatorQNN\n",
        "from qiskit_machine_learning.connectors import TorchConnector\n",
        "\n",
        "from qiskit.quantum_info import Statevector\n",
        "from qiskit.primitives import Sampler\n",
        "from qiskit_algorithms.state_fidelities import ComputeUncompute\n",
        "from qiskit_machine_learning.kernels import FidelityQuantumKernel\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "MJyYMXxrpNYI"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XPew3s5na-k",
        "outputId": "9bc6de3e-848e-4f0c-fc16-a609d550aef9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Datasets"
      ],
      "metadata": {
        "id": "L7CN4N5fnTCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Medquad (HCD) dataset"
      ],
      "metadata": {
        "id": "JgvaKHoMnn6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length=51\n",
        "df = pd.read_csv(r'/content/drive/MyDrive/LLM Healthcare/medquad.csv', usecols=['question', 'focus_area'])\n",
        "arr=df.to_numpy()\n",
        "# Label count and index\n",
        "label_count={}\n",
        "for i in arr:\n",
        "  label_count[i[-1]]=label_count.get(i[-1],0)+1\n",
        "labels_count=sorted(label_count.items(),key=lambda x:-x[1])\n",
        "label_index={i[0]:j for j,i in enumerate(labels_count)}\n",
        "#Word count and index\n",
        "word_counts={}\n",
        "for i in arr:\n",
        "  for j in i[0].split():\n",
        "    word_counts[j]=word_counts.get(j,0)+1\n",
        "word_counts=sorted(word_counts.items(),key=lambda x:-x[1])\n",
        "word_index={i[0]:j+1 for j,i in enumerate(word_counts)}\n",
        "word_index['PAD']=0\n",
        "word_index=dict(sorted(word_index.items(),key=lambda x:x[1]))\n",
        "#Tokenization\n",
        "def tokenization(words):\n",
        "  token=[]\n",
        "  for i in words:\n",
        "    k=[]\n",
        "    for j in i.split():\n",
        "      k.append(word_index[j])\n",
        "    token.append(k)\n",
        "  return token\n",
        "# Tokenizing the data\n",
        "tokenized_data=tokenization([i[0] for i in arr])\n",
        "labels=[label_index[i[1]] for i in arr]\n",
        "#Padding the data\n",
        "data= keras.preprocessing.sequence.pad_sequences(tokenized_data,maxlen=max_length,padding=\"post\")\n",
        "#Only selecting two labels\n",
        "new_data,label=[],[]\n",
        "for i,j in zip(data,labels):\n",
        "  if j<2:\n",
        "    new_data.append(i)\n",
        "    label.append(j)\n",
        "combined_data=pd.DataFrame(np.c_[new_data,label])\n",
        "h_data=new_data\n",
        "h_label= label\n",
        "combined_data.columns = list(combined_data.columns)[:-1] + ['Cluster']"
      ],
      "metadata": {
        "id": "AXlNDwicnQzb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = combined_data\n",
        "cluster = np.array(df['Cluster'])"
      ],
      "metadata": {
        "id": "umIyGw9SnxJR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = df.select_dtypes(include=np.number).columns\n",
        "all_zero_cols = [col for col in numeric_cols if df[col].sum() == 0]\n",
        "df_cleaned = df.drop(columns=all_zero_cols)"
      ],
      "metadata": {
        "id": "yUoxIIJBn62P"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_cleaned"
      ],
      "metadata": {
        "id": "FQ8q3ZMIn9Fo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row_arrays = []\n",
        "for index, row in df.iterrows():\n",
        "    row_arrays.append(row.to_numpy())"
      ],
      "metadata": {
        "id": "4yS0JNWIoA8v"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h_data = row_arrays"
      ],
      "metadata": {
        "id": "xUora7IToDg1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_qnn():\n",
        "    feature_map = ZZFeatureMap(2, reps= 3)\n",
        "    ansatz = RealAmplitudes(2, reps=3)\n",
        "\n",
        "    ansatz = ansatz.assign_parameters({param: Parameter(f'phi[{i}]') for i, param in enumerate(ansatz.parameters)})\n",
        "\n",
        "    qc = QuantumCircuit(2)\n",
        "\n",
        "    qc.h(0)\n",
        "    qc.h(1)\n",
        "\n",
        "    qc.compose(feature_map, inplace=True)\n",
        "\n",
        "    qc.compose(ansatz, inplace=True)\n",
        "\n",
        "    qc.h(0)\n",
        "    qc.h(1)\n",
        "\n",
        "    qnn = EstimatorQNN(\n",
        "        circuit=qc,\n",
        "        input_params=feature_map.parameters,\n",
        "        weight_params=ansatz.parameters,\n",
        "        input_gradients=True,\n",
        "    )\n",
        "    return qnn"
      ],
      "metadata": {
        "id": "tryXF2H7oF-Z"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qnn4 = create_qnn()\n",
        "\n",
        "vocab_size_h=118\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from qiskit_machine_learning.connectors import TorchConnector\n",
        "\n",
        "class ImprovedNN(nn.Module):\n",
        "    def __init__(self, vocab_size_h, qnn4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_dim = 16\n",
        "        self.sequence_length = 13\n",
        "\n",
        "        self.embed = nn.Embedding(vocab_size_h, self.embedding_dim)\n",
        "\n",
        "        encoder_input_size = self.sequence_length * self.embedding_dim\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(encoder_input_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(64),\n",
        "            nn.Linear(64, 2),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.qnn = TorchConnector(qnn4)\n",
        "        self.post_qnn = nn.Sequential(\n",
        "            nn.Linear(1, 1),\n",
        "            nn.Sigmoid()                 # Binary classification\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = self.encoder(x)\n",
        "        x = self.qnn(x)\n",
        "\n",
        "        x = self.post_qnn(x)                   # Final classifier layer\n",
        "        return x.view(-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56cjzP6PpXWS",
        "outputId": "9a438e6f-2a78-4a0b-9561-895be6837c1c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2108818143.py:26: DeprecationWarning: V1 Primitives are deprecated as of qiskit-machine-learning 0.8.0 and will be removed no sooner than 4 months after the release date. Use V2 primitives for continued compatibility and support.\n",
            "  qnn = EstimatorQNN(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming h_data and h_label are numpy arrays\n",
        "m_data_tensor = torch.tensor(h_data, dtype=torch.long)\n",
        "m_data_labels = torch.tensor(h_label, dtype=torch.float32)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    m_data_tensor,\n",
        "    m_data_labels,\n",
        "    test_size=0.2,       # 20% validation\n",
        "    shuffle=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "epochs = 50\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "loss_list = []\n",
        "model = ImprovedNN(vocab_size_h, qnn4)\n",
        "optimizer = torch.optim.Adadelta(model.parameters(), lr=0.01, weight_decay=0.8)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f'For epoch: {epoch+1}/{epochs}')\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(X_train)\n",
        "    loss = loss_fn(output, y_train)\n",
        "    loss_list.append(loss)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        acc = (output > 0.5).int().eq(y_train.int()).float().mean().item()\n",
        "        if acc == 1.0:\n",
        "            break\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    preds = (model(X_val) > 0.5).int()\n",
        "    y_val_np = y_val.int().numpy()\n",
        "    preds_np = preds.numpy()\n",
        "\n",
        "    accuracy = accuracy_score(y_val_np, preds_np)\n",
        "    precision = precision_score(y_val_np, preds_np)\n",
        "    recall = recall_score(y_val_np, preds_np)\n",
        "    f1 = f1_score(y_val_np, preds_np)\n",
        "\n",
        "\n",
        "print(f\"Accuracy : {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall   : {recall}\")\n",
        "print(f\"F1 Score : {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxMbmvegyUUQ",
        "outputId": "f36b7268-71c3-4488-c012-d464841736f4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For epoch: 1/50\n",
            "For epoch: 2/50\n",
            "For epoch: 3/50\n",
            "For epoch: 4/50\n",
            "For epoch: 5/50\n",
            "For epoch: 6/50\n",
            "For epoch: 7/50\n",
            "For epoch: 8/50\n",
            "For epoch: 9/50\n",
            "For epoch: 10/50\n",
            "For epoch: 11/50\n",
            "For epoch: 12/50\n",
            "For epoch: 13/50\n",
            "For epoch: 14/50\n",
            "For epoch: 15/50\n",
            "For epoch: 16/50\n",
            "For epoch: 17/50\n",
            "For epoch: 18/50\n",
            "For epoch: 19/50\n",
            "For epoch: 20/50\n",
            "For epoch: 21/50\n",
            "For epoch: 22/50\n",
            "For epoch: 23/50\n",
            "For epoch: 24/50\n",
            "For epoch: 25/50\n",
            "For epoch: 26/50\n",
            "For epoch: 27/50\n",
            "For epoch: 28/50\n",
            "For epoch: 29/50\n",
            "For epoch: 30/50\n",
            "For epoch: 31/50\n",
            "For epoch: 32/50\n",
            "For epoch: 33/50\n",
            "For epoch: 34/50\n",
            "For epoch: 35/50\n",
            "For epoch: 36/50\n",
            "For epoch: 37/50\n",
            "For epoch: 38/50\n",
            "For epoch: 39/50\n",
            "For epoch: 40/50\n",
            "For epoch: 41/50\n",
            "For epoch: 42/50\n",
            "For epoch: 43/50\n",
            "For epoch: 44/50\n",
            "For epoch: 45/50\n",
            "For epoch: 46/50\n",
            "For epoch: 47/50\n",
            "For epoch: 48/50\n",
            "For epoch: 49/50\n",
            "For epoch: 50/50\n",
            "Accuracy : 0.95\n",
            "Precision: 0.9090909090909091\n",
            "Recall   : 1.0\n",
            "F1 Score : 0.9523809523809523\n"
          ]
        }
      ]
    }
  ]
}